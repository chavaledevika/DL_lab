//cell1
import numpy as np
import matplotlib.pyplot as plt

//cell2
#These are XOR inputs
x=np.array([[0,0,1,1],[0,1,0,1]])
#These are XOR outputs
y=np.array([[0,1,1,0]])

//cell3
#Number of inputs
nx = 2
#Number of neurns in output
ny = 1

//cell4
#Number of neurns in hidden layer
nh = 2

//cell5
#total training examples
m = x.shape[1]

//cell6
#learning rate
lr = 0.1

//cell7
#define random seed for consistent results
#np.random.seed(2)
#define weight matrices for neural network
w1 = np.random.rand(nh,nx)
w2 = np.random.rand(ny,nh)

//cell8
#I didnt use bias units
#We will use this list to accumulate losses
losses = []

//cell9
#Sigmoid activation function for hidden layer and output
def sigmoid(z):
z= 1/(1+np.exp(-z))
return z

//cell10
#forward propagation
def forward_prop(w1,w2,x):
z1 = np.dot(w1,x)
a1 = sigmoid(z1)
z2 = np.dot(w2,a1)
a2 = sigmoid(z2)
return z1,a1,z2,a2

//cell11
#backward propagation
def back_prop(m,w1,w2,z1,a1,z2,a2,y):
dz2 = a2-y
dw2 = np.dot(dz2,a1.T)/m
dz1 = np.dot(w2.T,dz2)*a1*(1-a1)
dw1 = np.dot(dz1,x.T)
return dz2,dw2,dz1,dw1

//cell12
iterations = 10000
for i in range(iterations):
z1,a1,z2,a2=forward_prop(w1,w2,x)
loss = -(1/m)*np.sum(y*np.log(a2)+(1-y)*np.log(1-a2))
losses.append(loss)
da2,dw2,dz1,dw1 = back_prop(m,w1,w2,z1,a1,z2,a2,y)
w2 = w2-lr*dw2
w1 = w1-lr*dw1

//cell13
plt.plot(losses)
plt.xlabel("EPOCHS")
plt.ylabel("Loss Value")

//cell14
def predict(w1,w2,input):
z1,a1,z2,a2=forward_prop(w1,w2,test)
a2=np.squeeze(a2)
if a2>=0.5:
print("For Input", [i[0] for i in input],"Output is 1")
else:
print("For Input", [i[0] for i in input],"Output is 0")

//cell15
test = np.array([[1],[0]])
predict(w1,w2,test)
test = np.array([[0],[0]])
predict(w1,w2,test)
test = np.array([[0],[1]])
predict(w1,w2,test)
test = np.array([[1],[1]])
predict(w1,w2,test)
