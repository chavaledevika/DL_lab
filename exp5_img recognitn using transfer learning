//cell1
import tensorflow_datasets as tfds
import tensorflow as tf

//cell2
# Load the dataset and split into train and test sets (70% train, 30% test)
(train_ds, train_labels), (test_ds, test_labels) = tfds.load(
"tf_flowers",
split=["train[:70%]", "train[70%:]"],
batch_size=-1,
as_supervised=True # Returns data as (image, label) tuples
)

//cell3
# Resize all images to 150x150 to match VGG16 input size
train_ds = tf.image.resize(train_ds, (150, 150))
test_ds = tf.image.resize(test_ds, (150, 150))

//cell4
# Preprocess images for VGG16 (mean subtraction)
train_ds = preprocess_input(train_ds)
test_ds = preprocess_input(test_ds)

//cell5
# Load the base model (VGG16 without top layers)
base_model = VGG16(weights="imagenet", include_top=False, input_shape=train_ds[0].shape)

//cell6
# Freeze the base model to prevent it from being trained
base_model.trainable = False

//cell7
# Build the final model by adding custom layers on top of VGG16
model = models.Sequential([
base_model, # VGG16 as feature extractor
layers.Flatten(), # Flatten the 3D output to 1D
layers.Dense(50, activation="relu"), # Dense layer with 50 units
layers.Dense(20, activation="relu"), # Dense layer with 20 units
layers.Dense(5, activation="softmax") # Output layer (5 classes for flowers)
])

//cell8
# Print model summary to view the architecture
model.summary()

//cell9
# Compile the model with sparse categorical cross-entropy loss and Adam optimizer
model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

//cell10
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input

//cell11
base_model=VGG16(weights="imagenet",include_top=False,input_shape=train_ds[0].shape)
base_model.trainable=False
train_ds=preprocess_input(train_ds)
test_ds=preprocess_input(test_ds)

//cell12
from tensorflow.keras import layers,models
flatten_layer=layers.Flatten()
dense_layer_1=layers.Dense(50,activation="relu")
dense_layer_2=layers.Dense(20,activation="relu")
prediction_layer=layers.Dense(5,activation="softmax")
model=models.Sequential([base_model,flatten_layer,dense_layer_1,dense_layer_2,prediction_layer])

//cell13
from tensorflow.keras.callbacks import EarlyStopping
model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])
model.fit(train_ds,train_labels,epochs=2)

//cell14
model.predict(test_ds[:2])

//cell15
# Train the model and store the history
history = model.fit(train_ds, train_labels, epochs=2, validation_data=(test_ds, test_labels))

//cell16
# Plot training and validation loss
import matplotlib.pyplot as plt
plt.figure(figsize=(8, 6))
plt.plot(history.history['loss'], label='Training Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Training and Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.show()
