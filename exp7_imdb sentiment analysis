//cell1
import tensorflow as tf
from tensorflow.keras.datasets import imdb
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

1.Load the dataset
//cell2
vocab_size = 10000
maxlen = 200
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)

2. Setup word index and decoder function
//cell3
word_index = imdb.get_word_index()
reverse_word_index = {value + 3: key for key, value in word_index.items()}
reverse_word_index[0] = "<PAD>"
reverse_word_index[1] = "<START>"
reverse_word_index[2] = "<UNK>"
reverse_word_index[3] = "<UNUSED>"

def decode_review(encoded_review):
    return ' '.join([reverse_word_index.get(i, '?') for i in encoded_review])

sample_index = 0
encoded_sample = x_train[sample_index]
decoded_sample = decode_review(encoded_sample)
vocab_size = 10000
maxlen = 200
(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)

print("Encoded Review: ")
print(encoded_sample)
print("\nDecoded Review: ")
print(decoded_sample)
print("\nSentiment: ", "Positive" if y_train[sample_index] == 1 else "Negative")

3. Preprocess the data
//cell4
x_train = pad_sequences(x_train, maxlen=maxlen, padding="post")
x_test = pad_sequences(x_test, maxlen=maxlen, padding="post")

4. Build the LSTM Model
model = Sequential([

Embedding(input_dim=vocab_size, output_dim=64, input_length=maxlen),
LSTM(100),
Dense(1, activation="sigmoid")
])

5. Compile the model
model.compile(optimizer="adam", loss="binary_crossentropy", metrics=["accuracy"])
# model.summary()
history = model.fit(x_train, y_train, epochs=3, batch_size=64, validation_split=0.2)

loss, accuracy = model.evaluate(x_test, y_test)
# print(f"Test Loss: {loss:.4f}")
print(f"Test Accuracy: {accuracy:.4f}")



